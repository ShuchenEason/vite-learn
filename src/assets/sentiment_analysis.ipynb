{"cells":[{"cell_type":"markdown","metadata":{"id":"bx4uNBJhDXoK"},"source":["**情感极性介绍**\n","\n","情感极性是指对一个事物、事件或者观点所持态度或情感色彩的正负程度。它是对情感的一种量化表示，用于描述情感是积极的、消极的还是中性的。情感极性在自然语言处理、社交媒体分析、舆情监测等领域中具有重要的应用。\n","\n","人类的情感极性是复杂而多样的，涵盖了愉快、悲伤、愤怒、惊喜、厌恶等各种情感状态。在情感分析中，常常将情感极性简化为积极（正面）、消极（负面）和中性三个类别。这种简化有助于对大量文本数据进行情感分类和情感倾向分析。\n","\n","情感极性的方法可以分为传统方法和基于机器学习/深度学习的方法。下面将介绍几种常见的情感极性分析方法：\n","\n","1、基于词典的方法：这种方法使用情感词典或词汇资源来标记文本中的情感词汇，并根据这些词汇的正负极性进行计算。每个情感词都被赋予一个情感得分，然后通过对得分求和或平均来确定文本的情感极性。这种方法简单直接，但对于新词或多义词可能无法准确处理。\n","\n","2、基于机器学习的方法：这种方法使用分类算法（如朴素贝叶斯、支持向量机等）或者特征提取方法（如TF-IDF、词袋模型等）来从文本中提取特征，并进行情感分类。通常需要有标注的训练数据来训练分类器，以学习情感特征和模式。这种方法可以考虑词汇之间的上下文关系，但对于模型训练和特征选择需要一定的数据和领域知识。\n","\n","3、基于深度学习的方法：这种方法使用深度神经网络模型，如卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆网络（LSTM）、注意力机制等来进行情感极性分析。这些模型可以通过学习文本的局部和全局特征，自动提取和理解情感表示。深度学习方法通常需要大量的标注数据和计算资源，但在处理复杂的情感表达和文本语义时具有较好的性能。\n","\n","4、结合多种方法的方法：有些研究工作结合了多种方法来进行情感极性分析，以综合考虑不同方法的优势。例如，可以结合词典方法和机器学习方法，利用词典进行情感词汇的标记，然后使用机器学习算法进行情感分类。这样可以兼顾传统方法的简单性和机器学习方法的准确性。\n","\n","情感极性的分析在实际应用中具有广泛的用途。在社交媒体分析中，可以用于监测用户对产品、品牌或事件的态度和意见。在舆情监测中，可以帮助政府、企业或组织了解公众对特定议题或事件的情感倾向。在市场营销中，可以评估消费者对产品或广告的喜好程度。此外，情感极性的分析还可以应用于情感机器人、智能客服和舆情预警等领域。\n","\n","不过，情感极性的分析依然面临一些挑战。由于语言的复杂性和多义性，情感识别和分类仍然是一个具有挑战性的任务。处理文本中的隐含情感、处理文化差异、处理讽刺或反讽等情感表达形式都是情感分析的难点之一。\n","\n","总而言之，情感极性是对情感的正负程度进行量化的一种表示。情感极性分析在多个领域中有重要应用，可以帮助人们理解和分析大量的文本数据中的情感态度，从而为决策和应用提供有价值的信息。"]},{"cell_type":"markdown","metadata":{"id":"Sp3Q48NwpOup"},"source":["**FineBERT介绍**\n","\n","FinBERT是一种专门用于金融领域的预训练语言模型。它是在Google的BERT（Bidirectional Encoder Representations from Transformers）模型的基础上进行微调和改进的。\n","\n","FinBERT模型的设计目标是能够更好地理解金融文本，并在金融领域的任务中提供更准确和有用的表示。与通用的BERT模型相比，FinBERT在预训练阶段使用了大量的金融文本数据，这包括新闻、公司报告、财务数据等。这使得FinBERT能够更好地理解金融领域的专业术语、表达方式和语境。\n","\n","为了进一步优化FinBERT在金融任务中的性能，研究人员还进行了特定领域的微调。微调是指使用特定任务的标注数据，通过在预训练模型上进行额外的训练，使其适应特定任务的要求。FinBERT提供了一系列针对不同金融任务的微调模型，如情感分析、ESG分析、前瞻性陈述识别等。\n","\n","FinBERT模型的优势在于它能够处理金融领域的专业术语和复杂语境，并提供对金融文本的更准确的表示。这使得它在金融市场预测、情感分析、舆情监测等任务中具有广泛的应用潜力。由于FinBERT是基于BERT模型的改进版本，它可以使用通用的预训练模型工具和技术，同时又针对金融领域进行了特定优化，为金融文本分析提供了一种强大的解决方案。"]},{"cell_type":"markdown","metadata":{"id":"bjKNh2AIPDta"},"source":["**Packages**\n","\n","\n","* transformers: transformers 提供 API 和工具来轻松下载和训练最先进的预训练模型。使用预训练模型可以降低计算成本、碳足迹，并节省从头开始训练模型所需的时间和资源。这些模型支持不同模式的常见任务，包括自然语言处理、计算机视觉、音频和多模态等。 具体资料可以参考：\n","  * paper: https://arxiv.org/pdf/1910.03771.pdf\n","  * github: https://github.com/huggingface/transformers\n","  * document: https://huggingface.co/docs/transformers/index\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ywbNCiECRkPv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m316.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting filelock (from transformers)\n","  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n","Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n","  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n","Collecting numpy>=1.17 (from transformers)\n","  Downloading numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/liuyusen5/Desktop/PROJECT_TRY/python/.conda/lib/python3.11/site-packages (from transformers) (24.0)\n","Collecting pyyaml>=5.1 (from transformers)\n","  Downloading PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n","Collecting regex!=2019.12.17 (from transformers)\n","  Downloading regex-2023.12.25-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m613.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting requests (from transformers)\n","  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n","Collecting tokenizers<0.19,>=0.14 (from transformers)\n","  Downloading tokenizers-0.15.2-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n","Collecting safetensors>=0.4.1 (from transformers)\n","  Downloading safetensors-0.4.2-cp311-cp311-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n","Collecting tqdm>=4.27 (from transformers)\n","  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n","  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/liuyusen5/Desktop/PROJECT_TRY/python/.conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Collecting charset-normalizer<4,>=2 (from requests->transformers)\n","  Downloading charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (33 kB)\n","Collecting idna<4,>=2.5 (from requests->transformers)\n","  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n","  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n","Collecting certifi>=2017.4.17 (from requests->transformers)\n","  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n","Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-macosx_10_9_x86_64.whl (187 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.9/187.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading regex-2023.12.25-cp311-cp311-macosx_10_9_x86_64.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.2-cp311-cp311-macosx_10_12_x86_64.whl (426 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-macosx_10_12_x86_64.whl (2.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n","Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl (121 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading idna-3.6-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n","Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2024.3.1 huggingface-hub-0.21.4 idna-3.6 numpy-1.26.4 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.39.1 urllib3-2.2.1\n"]}],"source":["#安装transformers库\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lyXyB0qnR5UT"},"outputs":[],"source":["from transformers import BertTokenizer, BertForSequenceClassification, pipeline"]},{"cell_type":"markdown","metadata":{"id":"FQOCEKfBSzNh"},"source":["**FinBERT Sentiment**\n","\n","分析金融文本情感是有价值的，因为它可以涵盖管理人员、信息中介和投资者的观点和意见。FinBERT-Sentiment是一个在标注了来自标准普尔500指数公司的分析师报告中的1万个句子的基础上进行微调的FinBERT模型。\n","\n","*   输入：一个金融文本\n","*   输出：积极、消极或中性"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4VqGHu5R7cD"},"outputs":[],"source":["finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n","tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n","\n","nlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\n","results = nlp('Growth is strong and we have plenty of liquidity.')\n","\n","print(results)  # [{'label': 'Positive', 'score': 1.0}]"]},{"cell_type":"markdown","metadata":{"id":"5069BOpeTcZi"},"source":["**FinBERT ESG**\n","\n","ESG分析可以帮助投资者确定企业的长期可持续性，并识别相关的风险。FinBERT-ESG是一个在企业的ESG报告和年度报告中手动标注的2,000个句子上进行微调的FinBERT模型。\n","\n","\n","\n","*   输入：一个金融文本\n","*   输出：环境、社会、治理或无\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Vv2FLQ8TZaX"},"outputs":[],"source":["finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-esg',num_labels=4)\n","tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-esg')\n","\n","nlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\n","results = nlp('Rhonda has been volunteering for several years for a variety of charitable community programs.')\n","\n","print(results) # [{'label': 'Social', 'score': 0.9906041026115417}]"]},{"cell_type":"markdown","metadata":{"id":"cRXGDZ8IUJUT"},"source":["**FinBERT FLS**\n","\n","前瞻性陈述（FLS）向投资者提供了管理人员对公司未来事件或业绩的信念和意见。从企业报告中识别出前瞻性陈述可以帮助投资者进行财务分析。FinBERT-FLS是一个在Russell 3000公司的年度报告中管理讨论与分析部分手动标注的3,500个句子上进行微调的FinBERT模型。\n","\n","*   输入：一个金融文本\n","*   输出：具体的前瞻性陈述（Specific-FLS）、非具体的前瞻性陈述（Non-specific FLS）或非前瞻性陈述（Not-FLS）。\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yf_iscAQUIGr"},"outputs":[],"source":["finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls',num_labels=3)\n","tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')\n","\n","nlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer)\n","results = nlp('We expect the age of our fleet to enhance availability and reliability due to reduced downtime for repairs.')\n","\n","print(results)  # [{'label': 'Specific FLS', 'score': 0.77278733253479}]"]},{"cell_type":"markdown","metadata":{"id":"MmnKbmu1by_Y"},"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMgfTKmbntRrmVdRnZMHdCq","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
